<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications on Rui Zhu</title>
    <link>https://MaureenZOU.github.io/publication/</link>
    <description>Recent content in Publications on Rui Zhu</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Rui Zhu</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/publication/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Rethinking Reprojection: Closing the Loop for Pose-aware Shape Reconstruction from a Single Image</title>
      <link>https://MaureenZOU.github.io/publication/6dof/</link>
      <pubDate>Sun, 22 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://MaureenZOU.github.io/publication/6dof/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://MaureenZOU.github.io/img/pubs/6dof.png&#34; alt=&#34;&#34; /&gt;
An emerging problem in computer vision is the reconstruction of 3D shape and pose of an object from a single image. Hitherto, the problem has been addressed through the application of canonical deep learning methods to regress from the image directly to the 3D shape and pose labels. These approaches, however, are problematic from two perspectives. First, they are minimizing the error between 3D shapes and pose labels - with little thought about the nature of this ``label error&amp;rdquo; when reprojecting the shape back onto the image. Second, they rely on the onerous and ill-posed task of hand labeling natural images with respect to 3D shape and pose. In this paper we define the new task of pose-aware shape reconstruction from a single image, and we advocate that cheaper 2D annotations of objects silhouettes in natural images can be utilized. We design architectures of pose-aware shape reconstruction which reproject the predicted shape back on to the image using the predicted pose. Our evaluation on several object categories demonstrates the superiority of our method for predicting pose-aware 3D shapes from natural images.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Object-Centric Photometric Bundle Adjustment with Deep Shape Prior</title>
      <link>https://MaureenZOU.github.io/publication/3dv2017/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://MaureenZOU.github.io/publication/3dv2017/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structure from Category: A Generic and Prior-less Approach</title>
      <link>https://MaureenZOU.github.io/publication/3dv2016/</link>
      <pubDate>Tue, 25 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://MaureenZOU.github.io/publication/3dv2016/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://MaureenZOU.github.io/img/pubs/3dv2016.png&#34; alt=&#34;&#34; /&gt;
&lt;strong&gt;Image credit to Chen Kong &lt;a href=&#34;http://www.cs.cmu.edu/~chenk//&#34; target=&#34;_blank&#34;&gt;http://www.cs.cmu.edu/~chenk//&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Inferring the motion and shape of non-rigid objects from images has been widely explored by Non-Rigid Structure from Motion (NRSfM) algorithms. Despite their promising results, they often utilize additional constraints about the camera motion (e.g. temporal order) and the deformation of the object of interest, which are not always provided in real-world scenarios. This makes the application of NRSfM limited to very few deformable objects (e.g. human face and body). In this paper, we propose the concept of Structure from Category (SfC) to reconstruct 3D structure of generic objects solely from images with no shape and motion constraint (i.e. prior-less). Similar to the NRSfM approaches, SfC involves two steps: (i) correspondence, and (ii) inversion. Correspondence determines the location of key points across images of the same object category. Once established, the inverse problem of recovering the 3D structure from the 2D points is solved over an augmented sparse shape-space model. We validate our approach experimentally by reconstructing 3D structures of both synthetic and natural images, and demonstrate the superiority of our approach to the state-of-the-art low-rank NRSfM approaches.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Conditional Lucas &amp; Kanade Algorithm</title>
      <link>https://MaureenZOU.github.io/publication/condlk/</link>
      <pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://MaureenZOU.github.io/publication/condlk/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://MaureenZOU.github.io/img/pubs/condLK.png&#34; alt=&#34;&#34; /&gt;
&lt;strong&gt;Image credit to Chen-Hsuan Lin &lt;a href=&#34;https://chenhsuanlin.bitbucket.io/&#34; target=&#34;_blank&#34;&gt;https://chenhsuanlin.bitbucket.io/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The Lucas &amp;amp; Kanade (LK) algorithm is the method of choice for efficient dense image and object alignment. The approach is efficient as it attempts to model the connection between appearance and geometric displacement through a linear relationship that assumes independence across pixel coordinates. A drawback of the approach, however, is its generative nature. Specifically, its performance is tightly coupled with how well the linear model can synthesize appearance from geometric displacement, even though the alignment task itself is associated with the inverse problem. In this paper, we present a new approach, referred to as the Conditional LK algorithm, which: (i) directly learns linear models that predict geometric displacement as a function of appearance, and (ii) employs a novel strategy for ensuring that the generative pixel independence assumption can still be taken advantage of. We demonstrate that our approach exhibits superior performance to classical generative forms of the LK algorithm. Furthermore, we demonstrate its comparable performance to state-of-the-art methods such as the Supervised Descent Method with substantially less training examples, as well as the unique ability to &amp;ldquo;swap&amp;rdquo; geometric warp functions without having to retrain from scratch. Finally, from a theoretical perspective, our approach hints at possible redundancies that exist in current state-of-the-art methods for alignment that could be leveraged in vision systems of the future.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
